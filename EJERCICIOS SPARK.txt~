EJERCICIOS SPARK

1. Arrancar el shell de spark para scala y familiarizarse con la información que
aparece por pantalla (Infos, Warnings, versión de Scala y Spark, etc...)
	$ spark-shell
	
	- version Spark 1.6.0
	- version Scala 2.10.5
	
2. Comprobar que se ha creado un contexto "sc" tal y como vimos en la
documentación.
	scala > sc
	res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@4634837c

3.Usando el comando de autocompletado csobre el SparkContext (sc), podéis ver
los métodos disponibles. La función de autocompletado consiste en presionar el
tab después de escribir el objeto SparkContext seguido de un punto.
	done
4.Para salir del Shell, escribir "exit" o presionar "Ctrl+C"

EXPLORACION DE FICHERO PLANO

1. Crea un RDD llamado “relato” que contenga el contenido del fichero
utilizando el método “textFile”

	scala > val textFile = sc.textFile("file:/home/BIT/data/relato.txt")
	# NO OLVIDAR PONER "file:" PARA TRABAJAR EN LOCAL
	# AUN NO SE HA CREADO EL RDD HAY QUE HACER UNA ACCION PRIMERO

2.Cuenta el número de líneas del RDD (una acción) y observa el resultado.
Si el resultado es 23 es correcto
	scala > textFile.count()
	res1: Long = 23

3.Ejecuta el método "collect()" sobre el RDD y observa el resultado. Recuerda lo
que comentamos durante el curso sobre cuándo es recomendable el uso de este método.

	~ Es recomendable cuando se trata de pequeños dataset
	scala > fileText.collect()
	res2: Array[String] = Array(Two roads diverged in a yellow wood,,
	And so...


4. Usando foreach, haz un display del archivo relato.txt de manera que sea más
fácil de leer.
	scala > textFile.foreach((linea:String)=>println(linea))



EXPLORACION DE FICHERO PLANO 2

1.Copia la carpeta weblogs contenida en la carpeta de ejercicios de Spark a “/home/BIT/
data/weblogs/” y revisa su contenido.

2. Escoge uno de los ficheros, ábrelo, y estudia cómo está estructurada cada una de sus
líneas (datos que contiene, separadores (espacio), etc)
	116.180.70.237 - 128 [15/Sep/2013:23:59:53 +0100] "GET /KBDOC-00031.html 	
	HTTP/1.0" 200 1388 "http://www.loudacre.com" "Loudacre CSR Browser"

3. 116.180.70.237 es la IP, 128 el número de usuario y GET /KBDOC-00031.html HTTP/1.0 el
artículo sobre el que recae la acción.

4. Crea una variable que contenga la ruta del fichero, por ejemplo file:/home/BIT/data/
weblogs/2013-09-15.log
	
	scala > val ruta = "file:/home/BIT/data/weblogs/2013-09-15.log"

5. Crea un RDD con el contenido del fichero llamada logs
	
	scala > val logs = sc.textFile(ruta)

6. Crea un nuevo RDD, jpglogs, que contenga solo las líneas del RDD que contienen la
cadena de caracteres “.jpg”. Puedes usar el método contains()

	scala > val jpglogs = logs.filter(x=>x.contains(".jpg"))

7. Imprime en pantalla las 5 primeras líneas de jpglogs
	
	scala > jpglogs.take(5)

8. Es posible anidar varios métodos en la misma línea. Crea una variable jpglogs2 que
devuelva el número de líneas que contienen la cadena de caracteres “.jpg”

	scala > val jpglogs2 = logs.filter(x=>x.contains(".jpg")).count()

9. Ahora vamos a comenzar a usar una de las funciones más importantes de Spark, la
función “map()”. Para ello, coge el RDD logs y calcula la longitud de las 5 primeras
líneas. Puedes usar la función “size()” o “length()” Recordad que la función map ejecuta
una función sobre cada línea del RDD, no sobre el conjunto total del RDD. 

	scala > logs.map(x=>x.length).take(5)

10. Imprime por pantalla cada una de las palabras que contiene cada una de las 5
primeras líneas del RDD logs. Puedes usar la función “split()”

	scala > logs.map(x=>x.split(" ")).take(5)

11. Mapea el contenido de logs a un RDD “logwords” de arrays de palabras de cada línea

	scala > val logwords = logs.map(x=>x.split(" "))

12. Crea un nuevo RDD llamado “ips” a partir del RDD logs que contenga solamente las
ips de cada línea (primer elemento de cada fila)

	scala > val ips = logs.map(linea=>linea.split(" ") (0))

13. Imprime por pantalla las 5 primeras líneas de ips

	scala > ips.take(5)

14. Visualiza el contenido de ips con la función “collect()”. Verás que no es demasiado
intuitivo. Prueba a usar el comando “foreach”
	scala > ips.collect()

	scala > ips.foreach(ip=>println(ip))

15. Crea un bucle “for” para visualizar el contenido de las 10 primeras líenas de ips.

	scala > for(linea<-ips.take(10)){println(linea)}

16. Guarda el contenido de “ips” entero en un fichero de texto usando el método
saveAsTextFile en la ruta “/home/cloudera/iplist” y observa su contenido.

	scala > ips.saveAsTextFile("file:/home/cloudera/ipList")

Exploración de un conjunto de ficheros planos en una carpeta

1.Crea un RDD que contenga solo las ips de todos los documentos contenidos en la ruta “/home/BIT/data/weblogs”. Guarda su contenido en la ruta “/home/cloudera/iplistw” y observa su contenido.
	
	scala > val allLogs = sc.textFile(ruta);
	scala > allLogs.map(linea=>linea.split(" ")(0)).saveAsTextFile("/home/cloudera/iplistw");


A partir del RDD logs, crea un RDD llamado “htmllogs” que contenga solo la ip seguida de cada ID de usuario de cada fichero html. El ID de usuario es el tercer campo de cada línea de cada log. Después imprime las 5 primeras líneas. Un ejemplo sería este:

	scala > val htmlLogs = allLogs.filter(_.contains(".html").map(linea=>(linea.split(" ")(0), linea.split(" ")(2)));

TRABAJANDO CON PAIR RDDs
Trabajo con todos los datos de la carpeta de logs: “/home/BIT/data/weblogs/*”

1.Usando MapReduce, cuenta el número de peticiones de cada usuario, es decir, las veces que cada usuario aparece en una línea de un log. Para ello

	a. Usa un Map para crear un RDD que contenga el par (ID, 1), siendo la clave el ID y el Value el número 1. Recordad que el campo ID es el tercer elemento de cada línea. Los datos obtenidos tendrían que quedar de la siguiente manera

	scala > var rdd = log.map(linea=>(linea.split(" ")(2), 1));

	b. Usa un Reduce para sumar los valores correspondientes a cada userid. Los datos tendría que mostrarse de la siguiente manera:

	scala > var numPet = rdd.reduceByKey((v1,v2)=>v1+v2);

2. Muestra los id de usuario y el número de accesos para los 10 usuarios con mayor número de accesos. Para ello:

	a. Utiliza un map() para intercambiar la Clave por el Valor, de forma que quede algo así: (Si no se te ocurre cómo hacerlo, busca en internet)
	
	scala > var numPetChange = numPet.map(field.swap());

	b. Utiliza la función vista en teoría para ordenar un RDD. Ten en cuenta que queremos mostrar los datos en orden descendiente (De mayor a menor número de peticiones). Recuerda que el RDD debe estar en la misma forma que al inicio, es decir, con clave: userid y valor: nº de peticiones.

	scala > for(x <- numPetChange.sortByeKey(false).take(10)){println(x)}

3.Crea un RDD donde la clave sea el userid y el valor sea una lista de ips a las que el userid se ha conectado (es decir, agrupar las IPs por userID). Ayúdate de la función groupByKey() para conseguirlo, de manera que el resultado final sea algo así:

	scala > var idip = log.map(linea=>(linea.split(" ")(2), linea.split(" ")(0))).groupByKey();

	Si te sobra tiempo prueba a mostrar el RDD resultante por pantalla de forma que tenga una estructura como la siguiente:

	ID:30000
	IPs:
	127.0.0.1
	155.124.124.1
	.
	.
	.

	scala > for(x<-idip.take(10)){println("ID:"+x._1);println("IPs:"); for(y<-x._2){println(y)}}

TRABAJO CON TODO LOS DATOS DE LA CARPETA DE LOGS: “/home/BIT/data/accounts.cvs”

1. Abre el fichero accounts.cvs con el editor de texto que prefieras y estudia su contenido. Verás que el primer campo es el id del usuario, que corresponde con el id del usuario de los logs del servidor web. El resto de campos corresponden con fecha, nombre, apellido, dirección, etc.

2. Haz un JOIN entre los datos de logs del ejercicio pasado y los datos de accounts.csv, de manera que se obtenga un conjunto de datos en el que la clave sea el userid y como valor tenga la información del usuario seguido del número de visitas de cada usuario. Los pasos a ejecutar son:

	a. Haz un map() de los datos de accounts.cvs de forma que la Clave sea el userid y el Valor sea toda la línea, incluido el userid. Obtendríamos algo de este tipo

	scala > val datosUsu = sc.textFile("file:/home/BIT/data/accounts.csv").map(linea=>(linea.split(",")).map(linea2 => (linea2(0), linea2))

	b. Haz un JOIN del RDD que acabas de crear con el que creaste en el paso anterior que contenía (userid, nº visitas), de manera que quede algo como esto:

	scala > val joinedRDD = datosUsu.join(numPet)

	c.Crea un RDD a patrir del RDD anterior, que contenga el userid, número de visitas, nombre y apellido de las 5 primeras líneas, para obtener una estructura como la siguiente (muestra más líneas de las requeridas):

	scala > for(pair<-joinedRDD.take(5)){println("("+pair._1+", "+ pair._2._2+", "+ pair._2._1(3)+", "+ pair._2._1(4)+")")}



C- Trabajo con más métodos sobre pares RDD

Tareas a realizar

1. Usa keyBy para crear un RDD con los datos de las cuentas, pero con el código postal como clave (noveno campo del fichero accounts.CSV). Puedes buscar información sobre este método en la API online de Spark

	scala > val keyedRDD = sc.textFile("file:/home/BIT/data/accounts.csv").map(line=>line.split(',')).keyBy(_(8))

2. Crea un RDD de pares con el código postal como la clave y una lista de nombres (Apellido, Nombre) de ese código postal como el valor. Sus lugares son el 5º y el 4º respectivamente.

	a. Si tienes tiempo, estudia la función “mapValues()” e intenta utilizarla para cumplir con el propósito de este ejercicio.

	
	scala > val nameByPCode = keyedRDD.mapValues(values => values(4)+","+values(3)).groupByKey()

EJERCICIOS ADICIONALES

1.	Toma el dataset 'shakespeare' proporcionado por el profesor. Cópialo a la máquina virtual arrastrando y soltando la carpeta.
2.Utilizando la terminal, introduce dicho dataset en el HDFS (La ruta por defecto del HDFS es: hdfs://quickstart.cloudera:8020/user/cloudera, copia el dataset en dicha ruta en la carpeta 'shakespeare')

	$ hadoop fs -put /ruta/a/shakespeare
	
3.Ahora vamos a realizar un análisis al estilo de escritura de Shakespeare, para ello nos interesa conocer las palabras más repetidas en sus obras. Muestra por pantalla las 100 palabras que más veces aparecen en las obras de Shakespeare, junto con la frecuencia de aparición de cada una, ordenadas descendientemente (de mayor a menor frecuencia de aparición).
	scala > val palabras = sc.textFile("shakespeare/*").flatMap(x => x.split(" "))
	scala > val cuentaPalabras = palabras.map(x => (x, 1)).reduceByKey((x, y) => x + y);
	scala > cuentaPalabras.take(20).foreach(println) 

4.El análisis anterior no es todo lo completo que una situación real nos puede requerir, pues entre las cpalabras más repetidas encontramos: pronombres, artículos, proposiciones...; es decir, palabras que no nos aportan información alguna. A este tipo de palabras se les conoce como palabras vacías (StopWords), y nos interesa eliminarlas de nuestro análisis.


5. Para la realización de este ejercicio vamos a necesitar un dataset que contenga "stop words" en inglés (pues, como es lógico, los libros de Shakespeare están en este idioma). Copia el archivo "stop-word-list.csv" en la máquina virtual y cópialo al HDFS (en la ruta por defecto).

6. También tenemos que tener en cuenta que, para hacer nuestro análisis, no nos interesan ni las líneas que solo contengan espacios en blanco ni las palabras que estén formadas por una única letra. Tampoco haremos distinción entre mayúsculas ni minúsculas, por lo que, por ejemplo, "Hello" y "hello" deberán ser tomadas como la misma palabra en el análisis.
	1.El método para cambiar una línea a minúsculas es: toLowerCase
	2.La expresión regular que toma únicamente caracteres alfabéticos es: "[^a-zA-Z]+"
	4.Un archivo CSV está delimitado por comas.
	5.No nos interesa mostrar las ocurrencias de una única letra. Por lo que deberemos filtrar el RDD final.

	
	scala > val logShakes = sc.textFile("shakespeare/*").flatMap(x => x.split(" ")).map(y => y.replaceAll("[^a-zA-Z]*", "")).map(z => z.toLowerCase)
	scala > val cuentaPalabras = logShakes.map(x => (x, 1))
	scala >	val stopWords = sc.textFile("stop-word-list.csv").flatMap(x=>x.split(",")).map( y => (y.trim,1))
	scala > val cuentaPalabras = logShakes.ap(x => (x,1))
	scala > val cuentaPalabras2 = cuentaPalabras.subtract(stopWords)
	scala > val cuentaPalabras3 = cuentaPalabras2.reduceByKey((x, y) => x + y)
	scala > val cuentaPalabras4 = cuentaPalabras3.map(x => x.swap).sortByKey(false).map(y => y.swap)
	scala > cuentaPalabras4.take(100).foreach(println)

EJERCICIOS SPARKSQL (JASON)
EL objetivo de este ejercicio es familiarizarnos con el uso de la herramienta SQL de Spark.

Tareas a realizar

1. Crea un nuevo contexto SQLContext
	scala > val sqc = new org.apache.spark.sql.SQLContext(sc)

2. Importa los implicits que permiten convertir RDDs en DataFrames
	scala > import sqlContext.implicits._
3. Carga el dataset “zips.json” que se encuentra en la carpeta de ejercicios de Spark y que contiene datos de códigos postales de Estados Unidos. Puedes usar el comando “ssc.load(“ruta_fichero”, “formato”)”. Tiene este aspecto:

	scala > val zips = sqc.load("file:/ruta/a/zips.jason", "json")

4. Visualiza los datos con el comando “show()”. Tienes que ver una tabla con 5 columnas con un subconjunto de los datos del fichero. Puedes ver que el código postal es “_id”, la ciudad es “city”, la ubicación “loc”, la población “pop” y el estado “state”.

	scala > zips.show

5. Obtén las filas cuyos códigos postales cuya población es superior a 10000 usando el api de DataFrames

	scala > zips.filter("pop > 10000").show

6. Guarda esta tabla en un fichero temporal para poder ejecutar SQL contra ella.

	scala > zips.registerTempTable("zips")

7. Realiza la misma consulta que en el punto 5, pero esta vez usando SQL
	
	scala > sqc.sql("SELECT * FROM zips WHERE pop > 10000").show

8. Usando SQL, obtén la ciudad con más de 100 códigos postales
	
	scala > sqc.sql("SELECT city FROM zips GROUP BY city HAVING COUNT(*) > 100").show

9. Usando SQL, obtén la población del estado de Wisconsin (WI)

	scala > sqc.sql("SELECT SUM(pop) AS population FROM zips WHERE state LIKE \"WI\" ").show

10. Usando SQL, obtén los 5 estados más poblados

	scala > sqc.sql("SELECT state, SUM(pop) AS population FROM zips GROUP BY state ORDER BY population DESC LIMIT 5").show


SPARKSQL(HIVE)
El objetivo de este ejercicio es familiarizarnos con el uso de SparkSQL para acceder a tablas en Hive, dado que es una herramienta ampliamente extendida en entornos analíticos.

Tareas a realizar

1. Abrir una terminal y ejecutad este comando: “sudo cp /usr/lib/hive/conf/hive-site.xml /usr/lib/spark/conf/”. Para más información podéis consultar esta web “ https://community.cloudera.com/t5/Advanced-Analytics-Apache-Spark/how-to-access-the-hive-tables-from-spark-shell/td-p/36609"

2. Reiniciar el shell de spark.

3. En un terminal arrancar el Shell de Hive y echar un vistazo a las bases de datos y tablas de cada una de ellas

4. En otro terminal aparte, arrancar el Shell de Spark, y a través de SparkSQL crear una base de datos y una tabla con dos o tres columnas. Si no creamocon Spark a través de Hive.
bbdd: hivespark
tabla: empleados
columnas: id INT, name STRING, age INT
config table: FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n'

	scala > import org.apache.spark.sql.hive.HiveContext
	scala > val sqc = new HiveContext(sc) //IMPORTANTE HiveContext
	scala > sqc.sql("CREATE DATABASE IF NOT EXISTS hivespark")
	scala > sqc.sql("CREATE TABLE IF NOT EXISTS cursohivedb.empleados (id INT,name STRING, age INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n'")

5. Crear un fichero “/home/cloudera/empleado.txt” que contenga los siguientes datos de esta manera dispuestos

6. Aprovechando la estructura de la tabla que hemos creado antes, usando SparkSQL, subid los datos del fichero “/home/cloudera/empleado.txt” a la tabla hive, usando como la sintaxis de HiveQL como vimos en el curso de Hive (LOAD DATA LOCAL INPATH).

	scala > sqc.sql("LOAD DATA LOCAL INPATH '/home/cloudera/empleados.txt' INTO TABLE hivespark.empleados")

7. Ejecutad cualquier consulta en los terminales de Hive y Spark para comprobar que todo funciona y se devuelven los mismos datos. En el terminal de Spark usad el comando “show()” para mostrar los datos.

	scala > sqc.sql(SELECT * FROM hivespark.empleados").show


SPARKSQL(DataFrames)

El objetivo de este ejercicio es familiarizarnos un poco más con la API de DataFrames.

1. Creamos un contexto SQL

	scala > val sqc = new org.apache.spark.sql.SQLContext(sc)

2. Importa los implicits que permiten convertir RDDs en DataFrames y Rows

	scala > import sqlContext.implicits._
	scala > import org.apache.spark.sql.Row
	scala > import org.apache.spark.sql.types.StructType
	scala > import org.apache.spark.sql.types.StructField
	scala > import org.apache.spark.sql.types.StringType

3. Creamos una variable con la ruta al fichero “/home/cloudera/Desktop/DataSetPartidos.txt”. Será necesario copiar el dataset “DataSetPartidos.txt” al escritorio de la máquina virtual.
	scala > val ruta = "file:/home/cloudera/Shared_Folder/data_spark/DataSetPartidos.txt"

4. Leemos el contenido del archivo en una variable

	scala >	val datosPartido = sc.textFile(ruta)

5. Creamos una variable que contenga el esquema de los datos

	scala > val schema = "id::temporada::jornada::EquipoLocal::EquipoVisitante::golesLocal::golesVisitante::fecha::timestamp"


6. Generamos el esquema basado en la variable que contiene el esquema de los datos que acabamos de crear

	scala > val schematic = StructType(schema.split("::").map(fieldName => StructField(fieldName, StringType, true)))

7. Convertimos las filas de nuestro RDD a Rows

	scala > val RowRDD = datosPartido.map(_.split("::")).map(p=>Row(p(0),p(1),p(2),p(3), p(4), p(5), p(6), p(7), p(8).trim))

8. Aplica el schema al RDD

	scala > val partidosDF = ssqc.createDataFrame(RowRDD, schematic)

9. Registramos el DataFrame como una Tabla

	scala > partidosDF.registerTempTable("partidos")

10. Ya estamos listos para hacer consultas sobre el DF con el siguiente formato

	scala > sqc.sql("SELECT COUNT(*) AS partidos_ganados_real_madrid FROM partidos WHERE (EquipoVisitante LIKE 'Real Madrid' AND golesVisitante > golesLocal) OR (EquipoLocal Like 'Real Madrid' AND golesLocal > golesVisitante)").show

	//Partidos ganados por el Real Madrid

11. los resulados de las queries son DF y soportan las operaciones como los RDDs normales. Las columnas en el Row de resultados son accesibles por índice o nombre de campo

	scala > val result = sqc.sql("SELECT COUNT(*) AS partidos_ganados_real_madrid FROM partidos WHERE (EquipoVisitante LIKE 'Real Madrid' AND golesVisitante > golesLocal) OR (EquipoLocal Like 'Real Madrid' AND golesLocal > golesVisitante)")
	scala > result.map(t => "Partidos ganados por el Real Madrid: " + t(0)).collect().foreach(println)

12. Ejercicio: ¿Cuál es el record de goles como visitante en una temporada del Oviedo?

	scala > sqc.sql("SELECT SUM(golesVisitante) FROM partidos WHERE EquipoVisitante LIKE '%Oviedo%' GROUP BY temporada ORDER BY SUM(golesVisitante) DESC").show

13. ¿Quién ha estado más temporadas en 1 Division Sporting u Oviedo?

	scala > sqc.sql("SELECT COUNT(DISTINCT(temporada)) FROM partidos WHERE EquipoLocal='Real Oviedo' OR EquipoVisitante='Real Oviedo'").show 

	scala > sqc.sql("SELECT COUNT(DISTINCT(temporada)) FROM partidos WHERE EquipoLocal='Sporting de Gijon' OR EquipoVisitante='Sporting de Gijon'").show 

	Sporting tiene mas temporadas en 1ra division :)

EJERCICIOS OPCIONALES SPARKSQL (Los Simpsons, sí, la serie animada) //TERMINAR MAS TARDE//

1. Toma el dataset 'simpsons_episodes.csv' proporcionado por el profesor. Cópialo a la máquina virtual arrastrando y soltando el archivo.

2. Utilizando la terminal, introduce dicho dataset en el HDFS (La ruta por defecto del HDFS es: hdfs://quickstart.cloudera:8020/user/cloudera, copia el dataset en dicha ruta.)

	$ hadoop fs -put /home/cloudera/Escritorio/simpsons.csv user/cloudera

3. El dataset que tenemos es estructurado, es decir, sus columnas tienen título (como en una base de datos). Invierte un rato en familiarizarte con el dataset y sus respectivos campos.

id,title,original_air_date,production_code,season,number_in_season,number_in_series, us_viewers_in_million,views,imdb_rating,imdb_votes,image_url,video_url


4. El objetivo de este ejercicio es conseguir un diagrama lineal en el que podamos apreciar, gráficamente, la puntuación media de los capítulos de los Simpsons durante todas sus temporadas. Este ejercicio constituye un posible caso real en el que podremos obtener información y realizar conclusiones mediante el análisis de multitud de datos.


5. Para cargar archivos ‘csv’ en Spark necesitamos utilizar esta librería, mientras que para crear el diagrama lineal a partir de los datos que obtengamos necesitamos esta otra. Para ejecutar Spark desde la terminal e indicarle las librería que queremos utilizar deberemos ejecutar el comando (IMPORTANTE EJECUTARLO SIN ESPACIOS):

	scala > 



