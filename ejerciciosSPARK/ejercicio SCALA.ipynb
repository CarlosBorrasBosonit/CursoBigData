{
  "metadata": {
    "name": "ejercicio SCALA",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.SparkSession\r\nimport org.apache.spark.sql.types._\r\n\r\n val spark \u003d SparkSession\r\n .builder\r\n .appName(\"Example-3_7\")\r\n .getOrCreate()\r\n\r\n // Get the path to the JSON file\r\n val jsonFile \u003d \"gs://ejercicios-bigdata1/notebooks/zeppelin/repo/blogs.json\"\r\n\r\n \r\n // Define our schema programmatically\r\n val schema \u003d StructType(Array(StructField(\"Id\", IntegerType, false),\r\n StructField(\"First\", StringType, false),\r\n StructField(\"Last\", StringType, false),\r\n StructField(\"Url\", StringType, false),\r\n StructField(\"Published\", StringType, false),\r\n StructField(\"Hits\", IntegerType, false),\r\n StructField(\"Campaigns\", ArrayType(StringType), false)))\r\n // Create a DataFrame by reading from the JSON file \r\n // with a predefined schema\r\n val blogsDF \u003d spark.read.schema(schema).json(jsonFile)\r\n\r\n // Show the DataFrame schema as output\r\n blogsDF.show(false)\r\n\r\n // Print the schema\r\n println(blogsDF.printSchema)\r\n println(blogsDF.schema)\r\n\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "blogsDF.take(20).foreach(println) //jjj"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "blogsDF.select(expr(\"Hits * 2\")).show(2)\n blogsDF.select(col(\"Hits\") * 2).show(2)"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "blogsDF.withColumn(\"Big Hitters\", (expr(\"Hits \u003e 10000\"))).show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.types._\n\n // Get the path to the CSV file\n val csvFile \u003d \"gs://ejercicios-bigdata1/notebooks/zeppelin/repo/sf-fire-calls.csv\"\n \n val df_fire_calls \u003d spark.read.options(Map(\"header\"-\u003e\"true\", \"inferSchema\"-\u003e\"true\",\"samplingRatio\"-\u003e\"0.001\")).csv(csvFile)\n\n df_fire_calls.schema.foreach(println)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// - What were all the different types of fire calls in 2018?\ndf_fire_calls.cache()\nval diff_types_of_calls \u003d df_fire_calls.select(\"CallType\").distinct()\n//diff_types_of_calls.show(truncate\u003dfalse)\n\n// - What months within the year 2018 saw the highest number of fire calls?\nval only_dates \u003d df_fire_calls.select(to_date(col(\"CallDate\"), \"MM/dd/yyyy\").as(\"date\")) // Primero hacemos un DataFrame donde convertimos la fechas a algo entendible por SQL\nonly_dates.createOrReplaceTempView(\"firecalls\")\nval months_with_most_calls \u003d spark.sql(\"SELECT MONTH(date) AS Month, COUNT(*) AS count FROM firecalls WHERE YEAR(date) LIKE 2018 GROUP BY MONTH ORDER BY COUNT(*) DESC\")\n\nval months_with_most_calls2 \u003d only_dates.filter(year($\"date\") \u003d\u003d\u003d 2018).groupBy(month($\"date\").as(\"Month\")).count().as(\"count\").orderBy(desc(\"count\"))\n//months_with_most_calls2.show(truncate\u003dfalse)\n//months_with_most_calls.show(truncate\u003dfalse)\n\n// - Which neighborhood in San Francisco generated the most fire calls in 2018?\n\nval callDate_neighborhoods_delay \u003d df_fire_calls.select(to_date(col(\"CallDate\"), \"MM/dd/yyyy\").as(\"date\"), $\"Neighborhood\", $\"Delay\")\n\ncallDate_neighborhoods_delay.createOrReplaceTempView(\"total_firecalls\")\n\nval most_fire_neighborhoods \u003d callDate_neighborhoods_delay\n                                            .select($\"Neighborhood\")\n                                            .filter(year($\"date\") \u003d\u003d\u003d 2018)\n                                            .groupBy($\"Neighborhood\")\n                                            .count().as(\"count\")\n                                            .orderBy(desc(\"count\"))\n//most_fire_neighborhoods.show(3, truncate\u003dfalse)\n\nval most_fire_neighborhoods_sql \u003d spark.sql(\"SELECT Neighborhood, COUNT(*) FROM total_firecalls WHERE YEAR(date) LIKE 2018 GROUP BY Neighborhood ORDER BY COUNT(*) DESC\")\n//most_fire_neighborhoods_sql.show(3, truncate\u003dfalse)\n\n\n// - Which neighborhoods had the worst response times to fire calls in 2018?\n\nval most_delay_neighborhoods \u003d callDate_neighborhoods_delay\n                                .where(year($\"date\") \u003d\u003d\u003d 2018)\n                                .groupBy($\"Neighborhood\")\n                                .agg(avg($\"Delay\").as(\"promedio\"))\n                                .orderBy(desc(\"promedio\"))\n                                \n//most_delay_neighborhoods.show(3)\n\nval most_delay_neighborhoods_sql \u003d spark.sql(\"SELECT Neighborhood, AVG(Delay) AS promedio FROM total_firecalls WHERE YEAR(date) LIKE 2018 GROUP BY Neighborhood ORDER BY AVG(Delay) DESC\")\n//most_delay_neighborhoods_sql.show(3, truncate\u003dfalse)\n\n// - Which week in the year in 2018 had the most fire calls?\nval most_fires_a_week \u003d callDate_neighborhoods_delay.select(weekofyear($\"date\").as(\"semana\")).where(year($\"date\") \u003d\u003d\u003d 2018).groupBy(\"semana\").count().as(\"count\").orderBy(desc(\"count\"))\n//most_fires_a_week.show(3, truncate \u003d false)\n\nval most_fires_a_week_sql \u003d spark.sql(\"SELECT WEEKOFYEAR(date) as Semana, COUNT(*) as Count FROM total_firecalls WHERE YEAR(date) LIKE 2018 GROUP BY Semana ORDER BY Count DESC\")\n//most_fires_a_week_sql.show(3, truncate \u003d false)\n\n// - Is there a correlation between neighborhood, zip code, and number of fire calls?\n\n\nval firecalls_for_corr \u003d df_fire_calls.select($\"Neighborhood\", $\"ZipCode\")\nfirecalls_for_corr.createOrReplaceTempView(\"firecalls_for_corr\")\nval dataFromCorr_sql \u003d spark.sql(\"SELECT Neighborhood, ZipCode, COUNT(*) as Count FROM firecalls_for_corr GROUP BY Neighborhood, ZipCode ORDER BY ZipCode DESC\")\ndataFromCorr_sql.show(truncate\u003dfalse)\n\n// - How can we use Parquet files or SQL tables to store this data and read it back?\n\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//Significa elementNullable \u003d true, quiere decir que ese elemento puede tomar un valor nulo"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//Dataset: es un conjunto de datos tipados \n//mientras que DataFrame es un conjunto de Datasets sin tipar\n//Para definir un Dataset, al ser un conjunto de datos enorme, inferir el schema puede llegar a ser costoso\n//por ello se utiliza \"case class\" para definir una clase con todos los tipos de cada columna del dataset"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.avro\n//JSON\nval jsonFolder \u003d \"gs://ejercicios-bigdata1/notebooks/zeppelin/repo/sf-fire-calls_json\"\nval jsonFile \u003d \"gs://ejercicios-bigdata1/notebooks/zeppelin/repo/sf-fire-calls.json\"\nprint(df_fire_calls.rdd.partitions.size+ \"\\n\")\ndf_fire_calls.coalesce(1).write.mode(\"overwrite\").json(jsonFile)\n\n//CSV (con distinto número)\nval csvFolder \u003d \"gs://ejercicios-bigdata1/notebooks/zeppelin/repo/sf-fire-calls-copy_csv\"\nval csvFile \u003d \"gs://ejercicios-bigdata1/notebooks/zeppelin/repo/sf-fire-calls-copy.csv\"\n\ndf_fire_calls.coalesce(1)\n   .write.format(\"com.databricks.spark.csv\").mode(\"overwrite\")\n   .option(\"header\", \"true\")\n   .save(csvFolder)\n   \n//AVRO\n//val avroFolder \u003d \"gs://ejercicios-bigdata1/notebooks/zeppelin/repo/sf-fire-calls_avro\"\n//val avroFile \u003d \"gs://ejercicios-bigdata1/notebooks/zeppelin/repo/sf-fire-calls.avro\"\n//df_fire_calls.write.mode(\"overwrite\")\n//df_fire_calls.write.format(\"avro\").save(avroFolder)"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// I. Se debe a que spark particiona el contenido.\n// II. Mediante df.rdd.partitions.(size o length)\n// III. Existe el método coalesce(n) o repartition(n)\n// IV. Ver ejercicio anterior."
    }
  ]
}