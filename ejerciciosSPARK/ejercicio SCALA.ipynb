{
  "metadata": {
    "name": "ejercicio SCALA",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//sc.textFile(\"gs://ejercicios-bigdata1/notebooks/zeppelin/repo/mnm.csv\").take(20).foreach(println)"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.SparkSession\r\nimport org.apache.spark.sql.types._\r\n\r\n val spark \u003d SparkSession\r\n .builder\r\n .appName(\"Example-3_7\")\r\n .getOrCreate()\r\n\r\n // Get the path to the JSON file\r\n val jsonFile \u003d \"gs://ejercicios-bigdata1/notebooks/zeppelin/repo/blogs.json\"\r\n // Define our schema programmatically\r\n val schema \u003d StructType(Array(StructField(\"Id\", IntegerType, false),\r\n StructField(\"First\", StringType, false),\r\n StructField(\"Last\", StringType, false),\r\n StructField(\"Url\", StringType, false),\r\n StructField(\"Published\", StringType, false),\r\n StructField(\"Hits\", IntegerType, false),\r\n StructField(\"Campaigns\", ArrayType(StringType), false)))\r\n // Create a DataFrame by reading from the JSON file \r\n // with a predefined schema\r\n val blogsDF \u003d spark.read.schema(schema).json(jsonFile)\r\n\r\n // Show the DataFrame schema as output\r\n blogsDF.show(false)\r\n\r\n // Print the schema\r\n println(blogsDF.printSchema)\r\n println(blogsDF.schema)\r\n\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "blogsDF.take(20).foreach(println) //jjj"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "blogsDF.select(expr(\"Hits * 2\")).show(2)\n blogsDF.select(col(\"Hits\") * 2).show(2)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "blogsDF.withColumn(\"Big Hitters\", (expr(\"Hits \u003e 10000\"))).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.types._\n\n // Get the path to the CSV file\n val csvFile \u003d \"gs://ejercicios-bigdata1/notebooks/zeppelin/repo/sf-fire-calls.csv\"\n \n val df_fire_calls \u003d spark.read.options(Map(\"header\"-\u003e\"true\", \"inferSchema\"-\u003e\"true\",\"samplingRatio\"-\u003e\"0.001\")).csv(csvFile)\n\n df_fire_calls.schema.foreach(println)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//Significa elementNullable \u003d true, quiere decir que ese elemento puede tomar un valor nulo"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//Dataset: es un conjunto de datos tipados \n//mientras que DataFrame es un conjunto de Datasets sin tipar\n//Para definir un Dataset, al ser un conjunto de datos enorme, inferir el schema puede llegar a ser costoso\n//por ello se utiliza \"case class\" para definir una clase con todos los tipos de cada columna del dataset"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.avro\n//JSON\nval jsonFolder \u003d \"gs://ejercicios-bigdata1/notebooks/zeppelin/repo/sf-fire-calls_json\"\nval jsonFile \u003d \"gs://ejercicios-bigdata1/notebooks/zeppelin/repo/sf-fire-calls.json\"\nprint(df_fire_calls.rdd.partitions.size+ \"\\n\")\ndf_fire_calls.coalesce(1).write.mode(\"overwrite\").json(jsonFile)\n\n//CSV (con distinto número)\nval csvFolder \u003d \"gs://ejercicios-bigdata1/notebooks/zeppelin/repo/sf-fire-calls-copy_csv\"\nval csvFile \u003d \"gs://ejercicios-bigdata1/notebooks/zeppelin/repo/sf-fire-calls-copy.csv\"\n\ndf_fire_calls.coalesce(1)\n   .write.format(\"com.databricks.spark.csv\").mode(\"overwrite\")\n   .option(\"header\", \"true\")\n   .save(csvFolder)\n   \n//AVRO\n//val avroFolder \u003d \"gs://ejercicios-bigdata1/notebooks/zeppelin/repo/sf-fire-calls_avro\"\n//val avroFile \u003d \"gs://ejercicios-bigdata1/notebooks/zeppelin/repo/sf-fire-calls.avro\"\n//df_fire_calls.write.mode(\"overwrite\")\n//df_fire_calls.write.format(\"avro\").save(avroFolder)"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// I. Se debe a que spark particiona el contenido.\n// II. Mediante df.rdd.partitions.(size o length)\n// III. Existe el método coalesce(n) o repartition(n)\n// IV. Ver ejercicio anterior."
    }
  ]
}