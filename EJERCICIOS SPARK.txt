EJERCICIOS SPARK

1. Arrancar el shell de spark para scala y familiarizarse con la información que
aparece por pantalla (Infos, Warnings, versión de Scala y Spark, etc...)
	$ spark-shell
	
	- version Spark 1.6.0
	- version Scala 2.10.5
	
2. Comprobar que se ha creado un contexto "sc" tal y como vimos en la
documentación.
	scala > sc
	res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@4634837c

3.Usando el comando de autocompletado csobre el SparkContext (sc), podéis ver
los métodos disponibles. La función de autocompletado consiste en presionar el
tab después de escribir el objeto SparkContext seguido de un punto.
	done
4.Para salir del Shell, escribir "exit" o presionar "Ctrl+C"

EXPLORACION DE FICHERO PLANO

1. Crea un RDD llamado “relato” que contenga el contenido del fichero
utilizando el método “textFile”

	scala > val textFile = sc.textFile("file:/home/BIT/data/relato.txt")
	# NO OLVIDAR PONER "file:" PARA TRABAJAR EN LOCAL
	# AUN NO SE HA CREADO EL RDD HAY QUE HACER UNA ACCION PRIMERO

2.Cuenta el número de líneas del RDD (una acción) y observa el resultado.
Si el resultado es 23 es correcto
	scala > textFile.count()
	res1: Long = 23

3.Ejecuta el método "collect()" sobre el RDD y observa el resultado. Recuerda lo
que comentamos durante el curso sobre cuándo es recomendable el uso de este método.

	~ Es recomendable cuando se trata de pequeños dataset
	scala > fileText.collect()
	res2: Array[String] = Array(Two roads diverged in a yellow wood,,
	And so...


4. Usando foreach, haz un display del archivo relato.txt de manera que sea más
fácil de leer.
	scala > textFile.foreach((linea:String)=>println(linea))



EXPLORACION DE FICHERO PLANO 2

1.Copia la carpeta weblogs contenida en la carpeta de ejercicios de Spark a “/home/BIT/
data/weblogs/” y revisa su contenido.

2. Escoge uno de los ficheros, ábrelo, y estudia cómo está estructurada cada una de sus
líneas (datos que contiene, separadores (espacio), etc)
	116.180.70.237 - 128 [15/Sep/2013:23:59:53 +0100] "GET /KBDOC-00031.html 	
	HTTP/1.0" 200 1388 "http://www.loudacre.com" "Loudacre CSR Browser"

3. 116.180.70.237 es la IP, 128 el número de usuario y GET /KBDOC-00031.html HTTP/1.0 el
artículo sobre el que recae la acción.

4. Crea una variable que contenga la ruta del fichero, por ejemplo file:/home/BIT/data/
weblogs/2013-09-15.log
	
	scala > val ruta = "file:/home/BIT/data/weblogs/2013-09-15.log"

5. Crea un RDD con el contenido del fichero llamada logs
	
	scala > val logs = sc.textFile(ruta)

6. Crea un nuevo RDD, jpglogs, que contenga solo las líneas del RDD que contienen la
cadena de caracteres “.jpg”. Puedes usar el método contains()

	scala > val jpglogs = logs.filter(x=>x.contains(".jpg"))

7. Imprime en pantalla las 5 primeras líneas de jpglogs
	
	scala > jpglogs.take(5)

8. Es posible anidar varios métodos en la misma línea. Crea una variable jpglogs2 que
devuelva el número de líneas que contienen la cadena de caracteres “.jpg”

	scala > val jpglogs2 = logs.filter(x=>x.contains(".jpg")).count()

9. Ahora vamos a comenzar a usar una de las funciones más importantes de Spark, la
función “map()”. Para ello, coge el RDD logs y calcula la longitud de las 5 primeras
líneas. Puedes usar la función “size()” o “length()” Recordad que la función map ejecuta
una función sobre cada línea del RDD, no sobre el conjunto total del RDD. 

	scala > logs.map(x=>x.length).take(5)

10. Imprime por pantalla cada una de las palabras que contiene cada una de las 5
primeras líneas del RDD logs. Puedes usar la función “split()”

	scala > logs.map(x=>x.split(" ")).take(5)

11. Mapea el contenido de logs a un RDD “logwords” de arrays de palabras de cada línea

	scala > val logwords = logs.map(x=>x.split(" "))

12. Crea un nuevo RDD llamado “ips” a partir del RDD logs que contenga solamente las
ips de cada línea (primer elemento de cada fila)

	scala > val ips = logs.map(linea=>linea.split(" ") (0))

13. Imprime por pantalla las 5 primeras líneas de ips

	scala > ips.take(5)

14. Visualiza el contenido de ips con la función “collect()”. Verás que no es demasiado
intuitivo. Prueba a usar el comando “foreach”
	scala > ips.collect()

	scala > ips.foreach(ip=>println(ip))

15. Crea un bucle “for” para visualizar el contenido de las 10 primeras líenas de ips.

	scala > for(linea<-ips.take(10)){println(linea)}

16. Guarda el contenido de “ips” entero en un fichero de texto usando el método
saveAsTextFile en la ruta “/home/cloudera/iplist” y observa su contenido.

	scala > ips.saveAsTextFile("file:/home/cloudera/ipList")

Exploración de un conjunto de ficheros planos en una carpeta

1.Crea un RDD que contenga solo las ips de todos los documentos contenidos en la ruta “/home/BIT/data/weblogs”. Guarda su contenido en la ruta “/home/cloudera/iplistw” y observa su contenido.
	
	scala > val allLogs = sc.textFile(ruta);
	scala > allLogs.map(linea=>linea.split(" ")(0)).saveAsTextFile("/home/cloudera/iplistw");


A partir del RDD logs, crea un RDD llamado “htmllogs” que contenga solo la ip seguida de cada ID de usuario de cada fichero html. El ID de usuario es el tercer campo de cada línea de cada log. Después imprime las 5 primeras líneas. Un ejemplo sería este:

	scala > val htmlLogs = allLogs.filter(_.contains(".html").map(linea=>(linea.split(" ")(0), linea.split(" ")(2)));

TRABAJANDO CON PAIR RDDs
Trabajo con todos los datos de la carpeta de logs: “/home/BIT/data/weblogs/*”

1.Usando MapReduce, cuenta el número de peticiones de cada usuario, es decir, las veces que cada usuario aparece en una línea de un log. Para ello

	a. Usa un Map para crear un RDD que contenga el par (ID, 1), siendo la clave el ID y el Value el número 1. Recordad que el campo ID es el tercer elemento de cada línea. Los datos obtenidos tendrían que quedar de la siguiente manera

	scala > var rdd = log.map(linea=>(linea.split(" ")(2), 1));

	b. Usa un Reduce para sumar los valores correspondientes a cada userid. Los datos tendría que mostrarse de la siguiente manera:

	scala > var numPet = rdd.reduceByKey((v1,v2)=>v1+v2);

2. Muestra los id de usuario y el número de accesos para los 10 usuarios con mayor número de accesos. Para ello:

	a. Utiliza un map() para intercambiar la Clave por el Valor, de forma que quede algo así: (Si no se te ocurre cómo hacerlo, busca en internet)
	
	scala > var numPetChange = numPet.map(field.swap());

	b. Utiliza la función vista en teoría para ordenar un RDD. Ten en cuenta que queremos mostrar los datos en orden descendiente (De mayor a menor número de peticiones). Recuerda que el RDD debe estar en la misma forma que al inicio, es decir, con clave: userid y valor: nº de peticiones.

	scala > for(x <- numPetChange.sortByeKey(false).take(10)){println(x)}

3.Crea un RDD donde la clave sea el userid y el valor sea una lista de ips a las que el userid se ha conectado (es decir, agrupar las IPs por userID). Ayúdate de la función groupByKey() para conseguirlo, de manera que el resultado final sea algo así:

	scala > var idip = log.map(linea=>(linea.split(" ")(2), linea.split(" ")(0))).groupByKey();

	Si te sobra tiempo prueba a mostrar el RDD resultante por pantalla de forma que tenga una estructura como la siguiente:

	ID:30000
	IPs:
	127.0.0.1
	155.124.124.1
	.
	.
	.

	scala > for(x<-idip.take(10)){println("ID:"+x._1);println("IPs:"); for(y<-x._2){println(y)}}

TRABAJO CON TODO LOS DATOS DE LA CARPETA DE LOGS: “/home/BIT/data/accounts.cvs”

1. Abre el fichero accounts.cvs con el editor de texto que prefieras y estudia su contenido. Verás que el primer campo es el id del usuario, que corresponde con el id del usuario de los logs del servidor web. El resto de campos corresponden con fecha, nombre, apellido, dirección, etc.

2. Haz un JOIN entre los datos de logs del ejercicio pasado y los datos de accounts.csv, de manera que se obtenga un conjunto de datos en el que la clave sea el userid y como valor tenga la información del usuario seguido del número de visitas de cada usuario. Los pasos a ejecutar son:

	a. Haz un map() de los datos de accounts.cvs de forma que la Clave sea el userid y el Valor sea toda la línea, incluido el userid. Obtendríamos algo de este tipo

	scala > val datosUsu = sc.textFile("file:/home/BIT/data/accounts.csv").map(linea=>(linea.split(",")).map(linea2 => (linea2(0), linea2))

	b. Haz un JOIN del RDD que acabas de crear con el que creaste en el paso anterior que contenía (userid, nº visitas), de manera que quede algo como esto:

	scala > val joinedRDD = datosUsu.join(numPet)

	c.Crea un RDD a patrir del RDD anterior, que contenga el userid, número de visitas, nombre y apellido de las 5 primeras líneas, para obtener una estructura como la siguiente (muestra más líneas de las requeridas):

	scala > for(pair<-joinedRDD.take(5)){println("("+pair._1+", "+ pair._2._2+", "+ pair._2._1(3)+", "+ pair._2._1(4)+")")}



C- Trabajo con más métodos sobre pares RDD

Tareas a realizar

1. Usa keyBy para crear un RDD con los datos de las cuentas, pero con el código postal como clave (noveno campo del fichero accounts.CSV). Puedes buscar información sobre este método en la API online de Spark

	scala > val keyedRDD = sc.textFile("file:/home/BIT/data/accounts.csv").map(line=>line.split(',')).keyBy(_(8))

2. Crea un RDD de pares con el código postal como la clave y una lista de nombres (Apellido, Nombre) de ese código postal como el valor. Sus lugares son el 5º y el 4º respectivamente.

	a. Si tienes tiempo, estudia la función “mapValues()” e intenta utilizarla para cumplir con el propósito de este ejercicio.

	
	scala > val nameByPCode = keyedRDD.mapValues(values => values(4)+","+values(3)).groupByKey()

EJERCICIOS ADICIONALES

1.	Toma el dataset 'shakespeare' proporcionado por el profesor. Cópialo a la máquina virtual arrastrando y soltando la carpeta.
2.Utilizando la terminal, introduce dicho dataset en el HDFS (La ruta por defecto del HDFS es: hdfs://quickstart.cloudera:8020/user/cloudera, copia el dataset en dicha ruta en la carpeta 'shakespeare')

	$ hadoop fs -put /ruta/a/shakespeare
	
3.Ahora vamos a realizar un análisis al estilo de escritura de Shakespeare, para ello nos interesa conocer las palabras más repetidas en sus obras. Muestra por pantalla las 100 palabras que más veces aparecen en las obras de Shakespeare, junto con la frecuencia de aparición de cada una, ordenadas descendientemente (de mayor a menor frecuencia de aparición).
	scala > val palabras = sc.textFile("shakespeare/*").flatMap(x => x.split(" "))
	scala > val cuentaPalabras = palabras.map(x => (x, 1)).reduceByKey((x, y) => x + y);
	scala > cuentaPalabras.take(20).foreach(println) 

⦁	El análisis anterior no es todo lo completo que una situación real nos puede requerir, pues entre las cpalabras más repetidas encontramos: pronombres, artículos, proposiciones...; es decir, palabras que no nos aportan información alguna. A este tipo de palabras se les conoce como ⦁	palabras vacías (StopWords), y nos interesa eliminarlas de nuestro análisis.

⦁	Para la realización de este ejercicio vamos a necesitar un dataset que contenga "stop words" en inglés (pues, como es lógico, los libros de Shakespeare están en este idioma). Copia el archivo "stop-word-list.csv" en la máquina virtual y cópialo al HDFS (en la ruta por defecto).

⦁	También tenemos que tener en cuenta que, para hacer nuestro análisis, no nos interesan ni las líneas que solo contengan espacios en blanco ni las palabras que estén formadas por una única letra. Tampoco haremos distinción entre mayúsculas ni minúsculas, por lo que, por ejemplo, "Hello" y "hello" deberán ser tomadas como la misma palabra en el análisis.
⦁	El método para cambiar una línea a minúsculas es: toLowerCase
⦁	La expresión regular que toma únicamente caracteres alfabéticos es: "[^a-zA-Z]+"
⦁	Un archivo CSV está delimitado por comas.
⦁	No nos interesa mostrar las ocurrencias de una única letra. Por lo que deberemos filtrar el RDD final.

⦁	El resultado de este ejercicio debería ser parecido a:

	









